<h1 id="il-lettore-google-cubo-unipol-1-aprile-2019">Il lettore Google (CUBO Unipol 1 aprile 2019)</h1>
<h3 id="questo-testo-accompagna-ma-non-sostituisce-le-slides-presentate-in-sala-disponibili-sempre-in-questo-repo">Questo testo accompagna ma non sostituisce le slides presentate in sala (disponibili sempre in questo repo)</h3>
<p>Negli utlimi anni abbiamo assistito a una crescita esponenziale della produzione di dati e documenti tra cui pagine Web, notizie giornalistiche, letteratura scientifica, e-mail, documenti aziendali e social media come blog, forum, recensioni di prodotti e tweet. Questi dati sono presenti sia all’interno delle reti aziendali, in ERP, CRM, ecc., sia sul web. Gli strumenti tipici della business intelligence e della gestione dei database incrementano e gestiscono questa “galassia” di dati. Per definizione la produzione editoriale è un’estrazione di questi dati, la loro analisi e sintesi da parte di un operatore esperto. Noi ci occuperemo di come questi dati possono a volte essere estratti e riutilizzati. Partiremo da semplici esempi di web scraping fino ad arrivare a individuare l’utilizzo del web semantico. Per seguire il titolo di questa conferenza potremmo dire che il “lettore google” è un lettore automatico di dati e testi e soprattutto di dati strutturati. Le tecnologie di cui parleremo sono solo di scraping e query. Non toccheremo tutte le tecnologie XML che consentono il recupero di questi dati e la loro pubblicazione. Perché questa strana idea della “galassia dei dati”? Un esempio: un’azienda ci chiede di pubblicare dati tratti in larga parte da un loro gestionale, ci viene consegnato un file XML ma gli autori insistono che per correggerlo devono averne una versione in word. In questo modo i dati non potranno più essere riutilizzati, escono dalla “galassia” e possono solo finire in una pubblicazione. Conservati in XML possono invece essere aggiornati e costituire materiale per future pubblicazioni.</p>
<p>Una delle attività di Google è raccogliere i link dei diversi siti. Ecco come si potrebbe fare: [NOTA: il lavoro principale di Google non è questo ma è rispondere alle domande degli utenti, risposte di cui questi link costituiscono solo una parte, il resto è profilazione delle caratteristiche dell’utente]</p>
<p>Salvare in un file link_crawler.py le seguenti righe</p>
<pre><code>from urllib.request import urlopen, urljoin
import re

def download_page(url):
    return urlopen(url).read().decode(&#39;utf-8&#39;)

def extract_links(page):
    link_regex = re.compile(&#39;&lt;a[^&gt;]+href=[&quot;\&#39;](.*?)[&quot;\&#39;]&#39;, re.IGNORECASE)
    return link_regex.findall(page)

if __name__ == &#39;__main__&#39;:
    target_url = &#39;https://clueb.it/&#39;
    clueb = download_page(target_url)
    links = extract_links(clueb)
    for link in links:
        print(urljoin(target_url, link))</code></pre>
<p>Da lanciare in questo modo:</p>
<pre><code>python3 link_crawler.py &gt; output.csv</code></pre>
<p>Per costruire un vero e proprio link crawler dovremmo memorizzare tutti i link che troviamo e poi passarli uno a uno alla ricerca dei link contenuti in quelle pagine in una sorta di ricerca ricorsiva. Naturalmente ci ricorderemo delle pagine già viste e non torneremo a visitarle se non dopo un certo periodo di tempo.</p>
<p>Scendiamo ora di livello ed entriamo in una pagina alla ricerca di quelli che abbiamo chiamato “dati”.</p>
<pre><code>&lt;!DOCTYPE html&gt; 
&lt;html&gt; 
    &lt;head&gt;
  &lt;meta charset=&quot;UTF-8&quot;&gt;
  &lt;title&gt; I Promessi Sposi&lt;/title&gt;
   &lt;/head&gt;
&lt;body&gt;
  &lt;div itemscope=&quot;&quot; itemtype=&quot;http://schema.org/Book&quot; itemprop=&quot;mainEntity&quot;&gt;
  &lt;img itemprop=&quot;image&quot; src=&quot;https://en.wikipedia.org/wiki/File:Francesco_Hayez_040.jpg&quot;
     alt=&quot;Alessandro Manzoni&quot;/&gt;
    &lt;span itemprop=&quot;name&quot;&gt;I promessi sposi&lt;/span&gt; —
    &lt;link itemprop=&quot;url&quot; href=&quot;https://it.wikipedia.org/wiki/I_promessi_sposi&quot; /&gt;&lt;br /&gt;
    di &lt;a itemprop=&quot;author&quot; href=&quot;https://it.wikipedia.org/wiki/Alessandro_Manzoni&quot;&gt;Alessandro Manzoni&lt;/a&gt;
  &lt;/div&gt;
  

    &lt;div itemtype=&quot;http://schema.org/Offer&quot; itemscope=&quot;&quot; itemprop=&quot;offers&quot;&gt;
      &lt;span itemprop=&quot;name&quot;&gt;I promessi sposi&lt;/span&gt;&lt;br /&gt;
      &lt;meta itemprop=&quot;priceCurrency&quot; content=&quot;EURO&quot; /&gt;
      &lt;span itemprop=&quot;price&quot;&gt;€ 19.95&lt;/span&gt;
      &lt;link itemprop=&quot;availability&quot; href=&quot;http://schema.org/InStock&quot;&gt;In Stock
    &lt;/div&gt;
   
  
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>
<p>Questo è un esempio di testo che fa uso di un linguaggio taggato molto comune: l’HTML5. E’ un caso particolare di XML (affermazione approssimativa ma sufficiente in questo contesto). Oltre a tag strutturali ne vediamo altri che individuano e illustrano aspetti contenutistici. E’ un esempio, <em>in nuce</em>, di web semantico.</p>
<p>Utilizziamo lxml in un esempio di web scraping. Prendiamo ad esempio una pagina di un sito di e-commerce</p>
<pre><code>from lxml import html
from lxml import etree
link = &#39;https://www.libreriauniversitaria.it/macroeconomia-prospettiva-europea-blanchard-olivier/libro/9788815265715&#39;
response = requests.get(link)
sourceCode = response.content
html_elem = html.fromstring(sourceCode)
for e in html_elem.xpath(&#39;//ul[@class=&quot;dettagli-prodotto&quot;]/li&#39;)]:
      print(e.text_content)

# Se avessimo utilizzato un parser XML avremmo ottenuto un errore perché, come spesso accade le pagine web non sono file XML &quot;ben formati&quot;, in genere vengono # dimenticati # i tag di chiusura
# root = etree.fromstring(response.content, base_url=&quot;http://where.it/is/from.xml&quot;) 
# genera un errore

# possono essere utilizzati anche i css selectors
from lxml.cssselect import CSSSelector
for e in htmlElem.cssselect(&quot;h1[itemprop]&quot;):
     print(e.text_content())

htmlElem.xpath(&#39;//h1[@itemprop=&quot;name&quot;]&#39;)[0].text_content()
# oppure anche
[e.get(&#39;itemprop&#39;) for e in htmlElem]


[ e.text_content() for e in htmlElem.xpath(&quot;//span[@itemprop]&quot;)]
[&#39;I promessi sposi&#39;, &#39;Alessandro Manzoni&#39;, &#39;I promessi sposi&#39;, &#39;€ 19.95&#39;]
</code></pre>
<p>Abbiamo scaricato una pagina html e ne abbiamo estratto i dati che cercavamo. Per individuarli abbiamo (manualmente) dovuto usare gli strumenti da sviluppatore presenti in tutti i principali browser. Trovato il dato che interessa scorrendo la pagina web, cliccate con il tasto destro su ‘Inspect element’ e ancora su ‘Copy XPath’</p>
<p>Sempre più spesso le pagine html includono “dati strutturati”. Questi dati possono essere estratti facilmente.</p>
<pre><code>import extruct
import requests
import pprint
from w3lib.html import get_base_url

r = requests.get(&#39;https://www.libreriauniversitaria.it/macroeconomia-prospettiva-europea-blanchard-olivier/libro/9788815265715&#39;)
base_url = get_base_url(r.text, r.url)
data = extruct.extract(r.content, base_url=base_url)
pp = pprint.PrettyPrinter(indent=2)

pp.pprint(data)
with open(&#39;libreria.json&#39;, &#39;w&#39;) as file:
    file.write(res)</code></pre>
<pre><code>&#39;https://www.nytimes.com/2019/03/07/us/politics/paul-manafort-sentencing.html?action=click&amp;module=Top%20Stories&amp;pgtype=Homepage&#39;

r = requests.get(&#39;https://www.nytimes.com/2019/03/14/science/pi-math-geometry-infinity.html&#39;)
base_url = get_base_url(r.text, r.url)
data = extruct.extract(r.content, base_url=base_url)
import json
res = json.dumps(data)
with open(&#39;nytimes.json&#39;, &#39;w&#39;) as file:
    file.write(res)</code></pre>
<p>Ora sappiamo cos’è un dato: una tripletta RDF: un soggetto, un predicato, un oggetto. Ciascuno di questi è in genere un URI.</p>
